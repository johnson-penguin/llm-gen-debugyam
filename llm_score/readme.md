## üìä Debug YAML Notes Scoring
This project provides an automated pipeline to evaluate and compare the quality of debug.yaml notes generated by RAG and Non-RAG workflows.
It uses an LLM (via NVIDIA API) to score each note based on clarity, correctness, reasoning, and relevance.

## üöÄ Features
- Extracts `notes` fields from YAML debug cases.
- Calls an LLM to **evaluate notes** with structured JSON output.
- Scoring criteria (0‚Äì2.5 each, total up to 10):
  1. **Clarity** ‚Äì readability of the note.
  2. **Technical Accuracy** ‚Äì correctness of the explanation.
  3. **Reasoning Quality** ‚Äì explains why, not only what.
  4. **Config Relevance** ‚Äì references to correct config parameters.
- Supports **RAG** and **None-RAG** generated YAML files.
- Consolidates all results into **CSV** + **JSON** + **visualization charts**.

## üìÇ Project Structure
```bash
debug_yaml_dir/                     # Input debug.yaml files
 ‚îú‚îÄ‚îÄ rag/                           # RAG-generated YAMLs
 ‚îî‚îÄ‚îÄ none_rag/                      # Non-RAG-generated YAMLs

scroe_debug_yaml.ipynb              # Main pipeline notebook

llm_scored_notes_with_scores.csv    # Scored notes with metadata
comparison_results.json             # Processed comparison results
merged_score_array.json             # Raw merged scoring array

llm_avg_score_comparison_bar.png    # Bar chart: avg score comparison
llm_rag_improvement_stats.png       # Statistics on RAG improvements
llm_score_comparison_with_diff.png  # Scatter plot: score comparison
llm_score_difference_distribution.png # Histogram: score differences

```


## ‚öôÔ∏è Workflow
### 1. **Read YAML files**
Extract `stage`, `type`, and `notes` fields.
### 2. **Build LLM prompt**
A prompt is constructed for each note with scoring instructions.
### 3. **Call NVIDIA API**
Using `microsoft/phi-4-multimodal-instruct`, the model outputs a JSON score:
```bash
{
  "score": 8.5,
  "comment": "The note is clear and technically correct, but lacks deep reasoning."
}

```
### 4. **Post-process results**
- Clean markdown wrappers (`json ...` ).
- Parse `score` and store evaluation results.

### 5. **Generate results**
- **CSV**: complete evaluation logs (`llm_scored_notes_with_scores.csv`).
- **JSON**: structured comparison data (`comparison_results.json`, `merged_score_array.json`).
- **Charts**: multiple plots for visual comparison of RAG vs Non-RAG.

  
## üìä Output Example
| file       | stage    | type | score | source   | comment                                           |
| ---------- | -------- | ---- | ----- | -------- | ------------------------------------------------- |
| case1.yaml | cu\_init | CU   | 8.0   | RAG      | "Clear explanation, correct config references."   |
| case2.yaml | cu\_init | CU   | 6.5   | None-RAG | "Readable but missing explanation of root cause." |

## üìà Visualizations

The notebook generates charts such as:
- **Average Score Comparison** (`llm_avg_score_comparison_bar.png`)
- **Improvement Statistics for RAG** (`llm_rag_improvement_stats.png`)
- **Score Comparison with Differences** (`llm_score_comparison_with_diff.png`)
- **Distribution of Score Differences** (`llm_score_difference_distribution.png`)


## ‚ñ∂Ô∏è Usage
### 1. Place your RAG and None-RAG .yaml files in:
- `debug_yaml_dir/rag/`
- `debug_yaml_dir/none_rag/`

### 2. Run the notebook:
```bash=
jupyter notebook scroe_debug_yaml.ipynb
```

### 3. Check results:

- **CSV**: `llm_scored_notes_with_scores.csv`
- **JSON**: structured score data
- **Charts**: PNG plots for comparison




